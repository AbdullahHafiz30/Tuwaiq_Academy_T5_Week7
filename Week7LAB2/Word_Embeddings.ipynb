{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Dlrp1nO-RQoZo6dHHhYUiPFh6ft9FiEZ","timestamp":1723294503179}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Introduction to Word Embeddings\n","\n","Word embeddings are a type of word representation that allows words to be represented as continuous vectors in a high-dimensional space. Unlike traditional representations like Bag of Words (BoW), word embeddings capture semantic meanings and relationships between words by placing similar words closer together in the vector space.\n","\n","### Key Concepts\n","\n","1. **Word Embedding**: A dense vector representation of a word where each dimension captures some aspect of its meaning.\n","2. **Pre-trained Embeddings**: Embeddings learned from large corpora, such as Word2Vec, GloVe, and FastText.\n","3. **Semantic Similarity**: Words with similar meanings will have similar embeddings, making it easier to perform tasks like word similarity and analogy."],"metadata":{"id":"ihBopOobLb2q"}},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n"],"metadata":{"id":"J14EKzDQLg2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Load pre-trained Word2Vec model (Google News vectors)\n","Note: This model is quite large. For demonstration, use a smaller or different model as needed.\n"," model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)"],"metadata":{"id":"6YBv0QyqLiSo"}},{"cell_type":"code","source":["# For demonstration, we'll use a smaller pre-trained model available in gensim\n","from gensim.downloader import load\n","model = load('glove-wiki-gigaword-50')"],"metadata":{"id":"Iyp8HlpbLmHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example words\n","words = ['king', 'queen', 'man', 'woman']"],"metadata":{"id":"xkigRBgHLoiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get embeddings\n","embeddings = {word: model[word] for word in words}"],"metadata":{"id":"xJngiE2qLqOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H01quYFFLWE0"},"outputs":[],"source":["# Display embeddings\n","for word, vector in embeddings.items():\n","    print(f\"Word: {word}\\nEmbedding: {vector}\\n\")"]},{"cell_type":"code","source":["# Find similar words\n","similar_words = model.most_similar('computer', topn=5)\n","\n","# TODO:: Display similar words"],"metadata":{"id":"rKCazO0NLsOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Solve analogy\n","analogy_result = model.most_similar(positive=['queen', 'man'], negative=['king'], topn=1)\n","\n","# TODO:: Display result"],"metadata":{"id":"fkcpJPKvL43C"},"execution_count":null,"outputs":[]}]}